{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e2a39e1-4f54-4d6a-b0cf-9378e9c68f0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Salting: The basic principle is to bucket the data in a group using a column with a random number -- the salt\n",
    "### This random key ensures that records with the same key are grouped together and spread evenly across partitions, preventing data skew and ensuring that each partition has a balanced workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4ee213-d446-4b77-bd2e-9c5d062648cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0956f1-d8bb-488a-8e46-b3487333dc33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/01 20:29:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"SaltingExample1\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c442da-6e66-4d11-ad4f-1a99d43cfb04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [(\"key1\", \"value1\"),\n",
    "        (\"key2\", \"value2\"),\n",
    "        (\"key3\", \"value3\"),\n",
    "        (\"key4\", \"value4\"),\n",
    "        (\"key5\", \"value5\"),\n",
    "        (\"key6\", \"value6\"),\n",
    "        (\"key7\", \"value7\"),\n",
    "        (\"key8\", \"value8\"),\n",
    "        (\"key9\", \"value9\"),\n",
    "        (\"key10\", \"value10\")]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "num_partitions = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61841c62-51cd-40d3-a1ca-7362880dfba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def salted_key(key):\n",
    "    return key + \"_\" + str(hash(key)%num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3502ec-e228-4183-98e8-4a1225c41a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "salted_rdd = rdd.map(lambda x: (salted_key(x[0]), x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e4e3b2-ccea-470a-ad69-b007ac977a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1_1', 'value1'),\n",
       " ('key2_0', 'value2'),\n",
       " ('key3_4', 'value3'),\n",
       " ('key4_1', 'value4'),\n",
       " ('key5_0', 'value5'),\n",
       " ('key6_1', 'value6'),\n",
       " ('key7_0', 'value7'),\n",
       " ('key8_1', 'value8'),\n",
       " ('key9_4', 'value9'),\n",
       " ('key10_4', 'value10')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salted_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa2d048-b07c-4e0e-a18e-00a94ad53305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_partitions(rdd, number_of_partitions):\n",
    "    partitions = rdd.partitionBy(number_of_partitions)\n",
    "    counter = 1\n",
    "    for partition in partitions.glom().collect():\n",
    "        print(f\"Partition{counter}:\")\n",
    "        counter+=1\n",
    "        for record in partition:\n",
    "            print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d58d76ad-aa66-49d3-878b-9cca690fe13f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Before salting=======\n",
      "Partition1:\n",
      "('key2', 'value2')\n",
      "('key5', 'value5')\n",
      "('key7', 'value7')\n",
      "Partition2:\n",
      "('key1', 'value1')\n",
      "('key4', 'value4')\n",
      "('key6', 'value6')\n",
      "('key8', 'value8')\n",
      "Partition3:\n",
      "Partition4:\n",
      "Partition5:\n",
      "('key3', 'value3')\n",
      "('key9', 'value9')\n",
      "('key10', 'value10')\n",
      "=======After salting=======\n",
      "Partition1:\n",
      "Partition2:\n",
      "('key2_0', 'value2')\n",
      "('key4_1', 'value4')\n",
      "('key5_0', 'value5')\n",
      "Partition3:\n",
      "('key1_1', 'value1')\n",
      "('key6_1', 'value6')\n",
      "Partition4:\n",
      "('key3_4', 'value3')\n",
      "('key7_0', 'value7')\n",
      "('key8_1', 'value8')\n",
      "('key9_4', 'value9')\n",
      "Partition5:\n",
      "('key10_4', 'value10')\n"
     ]
    }
   ],
   "source": [
    "print(\"=======Before salting=======\")\n",
    "show_partitions(rdd,5)\n",
    "print(\"=======After salting=======\")\n",
    "show_partitions(salted_rdd,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d471e5-49df-4424-844b-b904ae62699f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d55aeb8a-278f-4378-8310-25b978dbf68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "from pyspark.sql.functions import udf, concat, lit, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c249c75f-5cee-45dc-9292-165705362cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate example healthcare insurace claims dataset of fact and dimension\n",
    "def generate_fact_table(counter):\n",
    "    claims_records = []\n",
    "    drug_code = [\"D100\",\"D250\", \"D300\", \"D104\", \"D204\",\"D304\"]\n",
    "    claims_ids = [\"carrierXYZ\" + str(i) for i in range(1100, 1110)]\n",
    "    dayssupply_range = [i for i in range(5,100)] \n",
    "    for i in range(counter):\n",
    "        record = [i, random.choice(claims_ids), random.choice(drug_code), random.choice(dayssupply_range)] \n",
    "        claims_records.append(record) \n",
    "    return claims_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2cb92d1-9ca1-4eaa-8cfb-a88cf042f01d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fact_records = generate_fact_table(200)\n",
    "dimension_records = [[\"D100\",\"BrandA\"],\n",
    "                     [\"D250\",\"BrandB\"],\n",
    "                     [\"D300\",\"BrandC\"],\n",
    "                     [\"D104\",\"GenericA\"],\n",
    "                     [\"D204\",\"GenericB\"],\n",
    "                     [\"D304\",\"GenericC\"]] \n",
    "dimension_cols = [\"drug_code\",\"drug_name\"]\n",
    "fact_cols = [\"id\", \"claim_id\",\"drug_code\", \"dayssupply\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97e3f65e-53b3-4cad-98bc-8420a9f01b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"saltingExample2\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d0b13d-c766-4df5-a8d6-1bff3b1c43f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fact_df = spark.createDataFrame(data = fact_records, schema = fact_cols) \n",
    "dim_df = spark.createDataFrame(data = dimension_records, schema = dimension_cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67bfd10f-35dc-4a37-82aa-1b9a2d0184b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------+----------+\n",
      "| id|      claim_id|drug_code|dayssupply|\n",
      "+---+--------------+---------+----------+\n",
      "|  0|carrierXYZ1102|     D100|        89|\n",
      "|  1|carrierXYZ1107|     D300|        66|\n",
      "|  2|carrierXYZ1101|     D304|         8|\n",
      "|  3|carrierXYZ1101|     D104|        43|\n",
      "|  4|carrierXYZ1103|     D300|        10|\n",
      "|  5|carrierXYZ1102|     D100|        69|\n",
      "|  6|carrierXYZ1103|     D250|        34|\n",
      "|  7|carrierXYZ1101|     D100|        34|\n",
      "|  8|carrierXYZ1108|     D250|        96|\n",
      "|  9|carrierXYZ1100|     D204|        55|\n",
      "+---+--------------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3876c4e3-4160-4c2d-8485-df46d45b6ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec2f67ab-a76a-4e35-9d58-0024fab62dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined_df = fact_df.join(dim_df, on=\"drug_code\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d95a02b-f2c1-4f66-b9ee-fcdd619243cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rand():\n",
    "    return random.randint(0,4) \n",
    "udfrand = udf(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e99edea6-6ebc-46eb-9a03-83378bf84900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "salted_fact_df = fact_df.withColumn(\"skew_key\", lit(udfrand()))\n",
    "salted_dim_df = dim_df.withColumn(\"skew_key\", explode(lit([0,1,2,3,4])))\n",
    "salted_join_df = salted_fact_df.join(salted_dim_df, on=[\"drug_code\",\"skew_key\"], how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1098760-058a-4c8f-ba47-eb8794a63de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_in_each_partition(dataFrame):\n",
    "    partition_df = dataFrame.withColumn(\"partition_id\", spark_partition_id()).groupBy(\"partition_id\").agg(count(\"*\")).orderBy(\"partition_id\", ascending=True)\n",
    "    partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "558c38f5-4a8a-4b58-a479-ea7f51bd5c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|partition_id|count(1)|\n",
      "+------------+--------+\n",
      "|           0|      24|\n",
      "|           1|      34|\n",
      "|           2|      37|\n",
      "|           3|      34|\n",
      "|           4|      71|\n",
      "+------------+--------+\n",
      "\n",
      "+------------+--------+\n",
      "|partition_id|count(1)|\n",
      "+------------+--------+\n",
      "|           0|      71|\n",
      "|           1|      32|\n",
      "|           2|      36|\n",
      "|           3|      39|\n",
      "|           4|      22|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_in_each_partition(joined_df)\n",
    "count_in_each_partition(salted_join_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
